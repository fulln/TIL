---
dg-publish: true
title: 生产者压缩算法实现
createTime: 2023-06-24 21:24  
---


## 压缩算法

Kafka 的消息层次都分为两层：消息集合（message set）以及消息（message）。一个消息集合中包含若干条日志项（record item）

### v2 和v1 的区别

1.  把消息的公共部分抽取放到外层消息集合，这样就不用每条消息都保存这些信息
2. V1 版本中保存压缩消息的方法是把多条消息进行压缩然后保存到外层消息的消息体字段中；而 V2 版本的做法是对整个消息集合进行压缩。显然后者应该比前者有更好的压缩效果。


### 何时压缩

生产者和broker端

#### 生产者

props.put(“compression.type”, “gzip”)，它表明该 Producer 的压缩算法使用的是 GZIP。这样 Producer 启动后生产的每个消息集合都是经 GZIP 压缩过的，故而能很好地节省网络传输带宽以及 Kafka Broker 端的磁盘占用。

大部分情况下 Broker 从 Producer 端接收到消息后仅仅是原封不动地保存而不会对其进行任何修改

##### 有两种例外情况就可能让 Broker 重新压缩消息。

1. broker端指定了和producer端不同的压缩算法
一旦你在 Broker 端设置了不同的 compression.type 值，就一定要小心了，因为可能会发生预料之外的压缩 / 解压缩操作，通常表现为 Broker 端 CPU 使用率飙升。

2. Broker 端发生了消息格式转换。
为了兼容老版本的格式，Broker 端会对新版本消息执行向老版本格式的转换。这个过程中会涉及消息的解压缩和重新压缩。一般情况下这种消息格式转换对性能是有很大影响的，除了这里的压缩之外，它还让 Kafka 丧失了引以为豪的 Zero Copy 特性

### 何时解压缩

#### consumer
Producer 发送压缩消息到 Broker 后，Broker 照单全收并原样保存起来。当 Consumer 程序请求这部分消息时，Broker 依然原样发送出去，当消息到达 Consumer 端后，由 Consumer 自行解压缩还原成之前的消息。

1. Kafka 会将启用了哪种压缩算法封装进消息集合中

**Producer 端压缩、Broker 端保持、Consumer 端解压缩。**

#### broker

每个压缩过的消息集合在 Broker 端写入时都要发生解压缩操作，目的就是为了对消息执行各种验证。

##### broker 端校验
message set 层面，增加一个 crc，这样可以不用解压缩，直接校验压缩后的数据。 如果校验不成功，说明message set 中有损坏的message； 这时，再做解压操作，挨个校验message，找出损坏的那一个。


## 压缩算法对比


Kafka 2.1.0 版本之前，Kafka 支持 3 种压缩算法：GZIP、Snappy 和 LZ4。从 2.1.0 开始，Kafka 正式支持 Zstandard 算法（简写为 zstd）。

![](https://static001.geekbang.org/resource/image/cf/68/cfe20a2cdcb1ae3b304777f7be928068.png?wh=411*346)

在实际使用中，GZIP、Snappy、LZ4 甚至是 zstd 的表现各有千秋。但对于 Kafka 而言，它们的性能测试结果却出奇得一致，即在吞吐量方面：LZ4 > Snappy > zstd 和 GZIP；而在压缩比方面，zstd > LZ4 > GZIP > Snappy。具体到物理资源，使用 Snappy 算法占用的网络带宽最多，zstd 最少

### 何时启动压缩算法

1. 启用压缩的一个条件就是 Producer 程序运行机器上的 CPU 资源要很充足
2. 环境中带宽资源有限

# 地址

此文章为6月day24 学习笔记，内容来源于极客时间《https://time.geekbang.org/column/article/102132》